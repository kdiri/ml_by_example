{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "emails, labels = [], []\n",
    "#To load and label the spam email files to label '1'\n",
    "spam_file_path = 'datasets/enron1/spam'\n",
    "for filename in glob.glob(os.path.join(spam_file_path, '*.txt')):\n",
    "    with open(filename, 'r',  encoding= 'ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(1)\n",
    "\n",
    "#To load and label the non spam email files to label '0'\n",
    "ham_file_path = 'datasets/enron1/ham'\n",
    "for filename in glob.glob(os.path.join(ham_file_path, '*txt')):\n",
    "    with open(filename, 'r', encoding='ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(0)\n",
    "\n",
    "##Data preprocessing-\n",
    "##1. Number and punctuation removal\n",
    "##2. Human name removal\n",
    "##3. Stop words removal\n",
    "##4. Lemmatization\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def letters_only(astr): \n",
    "    return astr.isalpha()\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def cleanText(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(' '.join(lemmatizer.lemmatize(word.lower()) for word in doc.split()\n",
    "                             if letters_only(word) and word not in all_names))\n",
    "        #lowercase everything, isalpha does number and punc. removal, not in all_names removes words\n",
    "    return cleaned_docs\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "##print(\"\\t\\t---BEFORE CLEANING---\\n\\n\\n\")\n",
    "##print(emails[0])\n",
    "cleaned_emails = cleanText(emails)\n",
    "#print(\"\\n\\n\\n\\n\\t\\t---AFTER CLEANING---\\n\\n\")\n",
    "#print(cleaned_emails[:3])    #print the first 3 cleaned emails\n",
    "\n",
    "#to remove stop words use count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=500)\n",
    "\n",
    "#The vectorizer turns the document matrix into TERM DOCUMENT MATRIX where each row\n",
    "#is a term frequency sparse vector for a document and an email\n",
    "#The sparse vector is in the form of (row_index, feature/word_index) (word frequency)\n",
    "    #term_docs = cv.fit_transform(cleaned_emails)\n",
    "#print(term_docs[0])\n",
    "\n",
    "#To see the correspoding terms are from the feature_index do:\n",
    "#feature_names = cv.get_feature_names()\n",
    "#print(\"Word at index 481 is: {}\".format(feature_names[481]))   #put the index value as the argument    \n",
    "\n",
    "\n",
    "#---WITH ALL THE PREPROCESSING DONE, we can now build our Naive Bayes model---\n",
    "\n",
    "def get_label_index(labels):\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)    #label_index is a dictionary\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)        #appends the indices of the mails to 0 and 1 keys in the dict \n",
    "    return label_index\n",
    "\n",
    "#label_index = get_label_index(labels)\n",
    "\n",
    "def get_prior(label_index): \n",
    "    prior = {label:len(index) for label, index in label_index.items()}     #len index is just no. of emails under each 0, 1 category\n",
    "    print(prior)\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        #both labels are divided by the total\n",
    "        prior[label] /= float(total_count)    #x= x/n ::= x /= n\n",
    "    return prior    #dictionary with class label as key and corresponsiing prior as value\n",
    "\n",
    "\n",
    "prior = get_prior(label_index)\n",
    "print(prior)\n",
    "\n",
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    likelihood= {}    #empty dict; class : P(feature|class)\n",
    "    for label, index in label_index.items():\n",
    "        #calculate sum of 0 values and 1 values\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing    #smoothing used to adjust for 0 values\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)    #term frequency / total frequency of all terms\n",
    "        #likelihood[label] has #term frequency / total frequency or P(feature | class)  --- label [0] for ham, [1] for spam\n",
    "    return likelihood    #dictionary with class label as key and corresponding conditional P(feature|class) as value\n",
    "\n",
    "smoothing = 1\n",
    "#likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
    "#print(likelihood[0][:5])\n",
    "\n",
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "    #posterior is propostional to prior and likelihood\n",
    "    #posterior = exp(log(prior * likelihood))       #for easy calculation, faster.\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        for label, likelihood_label in  likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)   #take single row \n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                #if  log value is too large, assign it infinity\n",
    "                posterior[label] = float('inf')\n",
    "\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "            posteriors.append(posterior.copy())\n",
    "    return posteriors\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=42)\n",
    "#random_state gives the algorithm a seed to start at. Keeoing it fixed ensures the same split is obtained everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3465, 3465, 1707, 1707)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 984, 0: 2481}\n"
     ]
    }
   ],
   "source": [
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdiri/PycharmProjects/Training/venv/lib/python3.7/site-packages/ipykernel_launcher.py:122: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 21:24:40.608 | INFO     | __main__:<module>:16 - The accuracy on 1707 testing samples is: 30.228471001757466%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-17 at 21:24:40 | INFO | The accuracy on 1707 testing samples is: 30.228471001757466%\n",
      "2020-05-17 at 21:24:40 | INFO | The accuracy on 1707 testing samples is: 30.228471001757466%\n",
      "2020-05-17 at 21:24:40 | INFO | The accuracy on 1707 testing samples is: 30.228471001757466%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from loguru import logger\n",
    "logger.add(\n",
    "    sys.stdout,\n",
    "    colorize=True,\n",
    "    format=\"{time:YYYY-MM-DD at HH:mm:ss} | {level} | {message}\",\n",
    ")\n",
    "correct = 0.0\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "        elif pred[0] > 0.5:\n",
    "            correct += 1\n",
    "acc = correct/len(Y_test)\n",
    "logger.info(f'The accuracy on {len(Y_test)} testing samples is: {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior= True)\n",
    "clf.fit(term_docs_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 21:28:02.882 | INFO     | __main__:<module>:2 - The accuracy using MultinomialNB is 91.27123608670182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-17 at 21:28:02 | INFO | The accuracy using MultinomialNB is 91.27123608670182\n",
      "2020-05-17 at 21:28:02 | INFO | The accuracy using MultinomialNB is 91.27123608670182\n",
      "2020-05-17 at 21:28:02 | INFO | The accuracy using MultinomialNB is 91.27123608670182\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "logger.info(f\"The accuracy using MultinomialNB is {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1096,   95],\n",
       "       [  54,  462]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, prediction, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829443447037702"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "precision_score(Y_test, prediction, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953488372093024"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(Y_test, prediction, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611369990680335"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, prediction, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94      1191\n",
      "           1       0.83      0.90      0.86       516\n",
      "\n",
      "    accuracy                           0.91      1707\n",
      "   macro avg       0.89      0.91      0.90      1707\n",
      "weighted avg       0.92      0.91      0.91      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4c0b54a33793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction_prob' is not defined"
     ]
    }
   ],
   "source": [
    "pos_prob = prediction_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
